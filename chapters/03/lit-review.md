# Lit Review - Philosophy & Computers


## Computational Philosophy

  - Angius, Nicola & Tamburrini, Guglielmo (2011). Scientific Theories of Computational Systems  - in Model Checking. _Minds and Machines_ 21 (2):323-336.

  - Baek, Jongmin Jerome (2018). How to Solve Moral Conundrums with Computability Theory.  - _arXiv_.
  - Baumgaertner, Bert (2012). Vagueness Intuitions and the Mobility of Cognitive Sortals.   - _Minds and Machines_ 22 (3):213-234.
  - Blumson, Ben (2017). Anselm's God in Isabelle/HOL. _Archive of Formal Proofs_:9.
  - Bohr, Niels (1958). Unity of Knowledge. In _Atomic Physics and Human Knowledge_. Wiley. pp.  - 67--82.
  - Bourget, David (2010). Paperless Philosophy as a Philosophical Method. _Social Epistemology_   - 24 (4):363-375.
  - Brey, Philip (2005). The epistemology and ontology of human-computer interaction. _Minds and   - Machines_ 15 (3-4):383-398.
  - Dodig Crnkovic, Gordana & Stuart, Susan (eds.) (2007). _Computation, Information, Cognition:   - The Nexus and the Liminal_. Cambridge Scholars Press.
  - Dodig-Crnkovic, Gordana (2007). WHERE DO NEW IDEAS COME FROM? HOW DO THEY EMERGE? -  - EPISTEMOLOGY AS COMPUTATION. In Christian Calude (ed.), _Randomness & Complexity, from   - Leibniz to Chaitin_.
  - Fuenmayor, David & Benzmueller, Christoph, A Case Study on Computational Hermeneutics: E. J.   - Lowe’s Modal Ontological Argument.
  - Gustafsson, Johan E. & Peterson, Martin (2012). A Computer Simulation of the Argument from   - Disagreement. _Synthese_ 184 (3):387-405.
  - Hellie, Benj (2017). David Lewis and the Kangaroo: Graphing philosophical progress. In   - Russell Blackford & Damien Broderick (eds.), _Philosophy's Future: The Problem of   - Philosophical Progress_. Oxford: Blackwell.
  - Leibovitz, David Pierre (2013). _A Unified Cognitive Model of Visual Filling-In Based on an  - Emergic Network Architecture_. Dissertation, Carleton University
  - Lokhorst, Gert-Jan (2011). Computational Meta-Ethics. _Minds and Machines_ 21 (2):261-274.
  - Luczak-Roesch, Markus ; Tinati, Ramine ; Aljaloud, Saud ; Hall, Wendy & Shadbolt, Nigel  - (2016). A universal socio-technical computing machine. In _International Conference on Web   - Engineering_.
  - Mizrahi, Moti (forthcoming). What Isn’t Obvious about ‘obvious’: A Data-driven Approach to   - Philosophy of Logic. In Andrew Aberdein & Matthew Inglis (eds.), _Advances in Experimental  - Philosophy of Logic and Mathematics_. London: Bloomsbury Press.
  - Müller, Vincent C. (2008). What a course on philosophy of computing is not. _APA Newsletter  - on Philosophy and Computers_ 8 (1):36-38.
  - Oppenheimer, Paul & Zalta, Edward N. (2011). A computationally-discovered simplification of  - the ontological argument. _Australasian Journal of Philosophy_ 89 (2):333 - 349.
  - Prakken, Henry (2012). Reconstructing Popov v. Hayashi in a framework for argumentation with   - structured arguments and Dungean semantics. _Artificial Intelligence and Law_ 20 (1):57-82.
  - Primiero, Giuseppe (2012). A contextual type theory with judgemental modalities for  - reasoning from open assumptions. _Logique and Analyse_ 220:579-600.
  - Rooij, Iris ; Wright, Cory & Wareham, Todd (2012). Intractability and the use of heuristics  - in psychological explanations. _Synthese_ 187 (2):471-487.
  - Sergeyev, Yaroslav (2013). Solving ordinary differential equations by working with   - infinitesimals numerically on the Infinity Computer. _Applied Mathematics and Computation_  - 219 (22):10668–10681.
  - Sergeyev, Yaroslav & Garro, Alfredo (2013). Single-tape and multi-tape Turing machines   - through the lens of the Grossone methodology. _Journal of Supercomputing_ 65 (2):645-663.
  - Sloman, Aaron (1992). The emperor's real mind -- Review of Roger Penrose's The Emperor's new   - Mind: Concerning Computers Minds and the Laws of Physics. _Artificial Intelligence_ 56  - (2-3):355-396.
  - Tamariz, Monica (2011). Could arbitrary imitation and pattern completion have bootstrapped   - human linguistic communication? _Interaction Studies_ 12 (1):36-62.
  - Vakarelov, Orlin (2012). The Information Medium. _Philosophy and Technology_ 25 (1):47-65.
  - van Deemter, Kees ; Gatt, Albert ; van Gompel, Roger P. G. & Krahmer, Emiel (2012). Toward a   - Computational Psycholinguistics of Reference Production. _Topics in Cognitive Science_ 4  - (2):166-183.
  - Varenne, Franck (2013). The Nature of Computational Things. In Frédéric Migayrou Brayer &  Marie-Ange (eds.), _Naturalizing Architecture_. Orléans: HYX Editions. pp. 96-105.
  - Wheeler, Billy (2017). Humeanism and Exceptions in the Fundamental Laws of Physics.  - _Principia: An International Journal of Epistemology_ 21 (3):317-337.
  - Wheeler, Gregory (2016). Machine Epistemology and Big Data. In Lee McIntyre & Alex Rosenburg (eds.), _Routledge Companion to Philosophy of Social Science_. Routledge.
  - Wheeler, Gregory (2013). Models, Models, and Models. _Metaphilosophy_ 44 (3):293-300.




### André, Porto (2015). Semantical Mutation, Algorithms and Programs. _Dissertatio_:44-76.

@article{Andre2015-ANDSMA-4,
  title = {Semantical Mutation, Algorithms and Programs},
  year = {2015},
  pages = {44--76},
  author = {Porto Andr\'e},
  journal = {Dissertatio}
}

> This article offers an explanation of perhaps Wittgenstein’s strangest and least intuitive thesis – the semantical mutation thesis – according to which one can never answer a mathematical conjecture because the new proof alters the very meanings of the terms involved in the original question. Instead of basing our justification on the distinction between mere calculation and proofs of isolated propositions, characteristic of Wittgenstein’s intermediary period, we generalize it to include conjectures involving effective procedures as well. 

Highlights (p54) the idea that "methods of obtainment" for inferences & mathematical operations can be conceptualised as "programs".  

"A program can be seen as description of  an operation,  its definition.  More  then that:  differently from a simple description, one can even “run it”, so we can check if it’s correctly constructed, if everything is alright, no missteps involved). "

See (Martin Löf, 1984, p. 29) for a similar treatment of lambda calculus as computer programs.

Porto uses pseudo-code (p55) to formally demonstrate a linguistic claim.

"We do frame mathematical conjectures, and in the case of recursive operations the meanings of these conjectures could be very concretely construed as being about programs and their implementations, so even the fussy intuitionists would have to accept that." (p55)

"An algorithm, as well as a method, are both teleological notions. They are both means to a desired end." (p59)



### Ashton, Zoe & Mizrahi, Moti (2018). Show Me the Argument: Empirically Testing the Armchair Philosophy Picture. _Metaphilosophy_ 49 (1-2):58-70.

@article{Ashton2018-MIZSMT,
  author = {Zoe Ashton and Moti Mizrahi},
  pages = {58--70},
  journal = {Metaphilosophy},
  number = {1-2},
  year = {2018},
  volume = {49},
  title = {Show Me the Argument: Empirically Testing the Armchair Philosophy Picture}
}

> Many philosophers subscribe to the view that philosophy is a priori and in the business of discovering necessary truths from the armchair. This paper sets out to empirically test this picture. If this were the case, we would expect to see this reflected in philosophical practice. In particular, we would expect philosophers to advance mostly deductive, rather than inductive, arguments. The paper shows that the percentage of philosophy articles advancing deductive arguments is higher than those advancing inductive arguments, which is what we would expect from the vantage point of the armchair philosophy picture. The results also show, however, that the percentages of articles advancing deductive arguments and those advancing inductive arguments are converging over time and that the difference between inductive and deductive ratios is declining over time. This trend suggests that deductive arguments are gradually losing their status as the dominant form of argumentation in philosophy.

An empiric test of whether philosophy is 'a priori armchair conceptual analysis'.  A computer assisted research to parse a corpus of philosophical texts.

"In order to get an idea of how many philosophy articles employ deductive and inductive arguments, we searched the JSTOR database using the deductive and inductive indicator words outlined in table 2." (p62)

> To sum up, our key findings are the following: (p67)
>
>  1. The ratio, and likewise percentage, of philosophy articles advanc- ing deductive arguments is higher than the percentage of articles advancing inductive arguments.
>  2. The ratios and percentages of philosophy articles advancing deductive arguments and those advancing inductive arguments are converging over time.
>  3. The difference between inductive and deductive ratios, and their percentages, is declining over time.

"This trend suggests that deductive argu- ments are gradually losing their status as the dominant form of argumentation in philosophy." (p68)



### Ashton, Zoe & Mizrahi, Moti (2018). Intuition Talk is Not Methodologically Cheap: Empirically Testing the “Received Wisdom” About Armchair Philosophy. _Erkenntnis_ 83 (3):595-612.

@article{Ashton2018-ASHITI,
  number = {3},
  pages = {595--612},
  author = {Zoe Ashton and Moti Mizrahi},
  journal = {Erkenntnis},
  volume = {83},
  title = {Intuition Talk is Not Methodologically Cheap: Empirically Testing the \textquotedblleftReceived Wisdom\textquotedblright About Armchair Philosophy},
  year = {2018}
}

> The “received wisdom” in contemporary analytic philosophy is that intuition talk is a fairly recent phenomenon, dating back to the 1960s. In this paper, we set out to test two interpretations of this “received wisdom.” The first is that intuition talk is just talk, without any methodological significance. The second is that intuition talk is methodologically significant; it shows that analytic philosophers appeal to intuition. We present empirical and contextual evidence, systematically mined from the JSTOR corpus and HathiTrust’s Digital Library, which provide some empirical support for the second rather than the first hypothesis. Our data also suggest that appealing to intuition is a much older philosophical methodology than the “received wisdom” alleges. We then discuss the implications of our findings for the contemporary debate over philosophical methodology. 

Computer assisted empiric study of use of inuition in philosophical work.  Parsed a corpus of philosophical texts to extract key words (JSTOR’s Data for Research (dfr.jstor.org) and HathiTrust’s Digital Library (hathitrust.org))

"In this paper, we follow in Andow’s (2015a) footsteps methodologically, but we seek to advance the contemporary debate over philosophical methodology by focusing on the role that intuitions play in philosophical argumentation." (p597)

"We also used these two databases to search for phrases, such as ‘seems that’, ‘appears that’, and ‘it seems to me that’, which are phrases that are indicative of appeals to intuition, through philosophy publications in English." (p598)

> In this paper, through a systematic survey of the JSTOR and HathiTrust databases, we have found that: (p610)
>
> 1. Intuition talk (where indicators of intuition talk include ‘intuit’, ‘intuition, ‘intuitions’, ‘intuitive’, and ‘intuitively’) in philosophy is not as recent as the ‘‘received wisdom’’ alleges (circa 1960s); it goes as far back as the 1800s;
> 2. Nineteenth and twentieth century philosophers appealed to intuition (where indicators of appeals to intuition include ‘it seems that’, ‘it appears that’, and ‘it seems to me that’) in argumentative contexts;
> 3. The proportion of publications in which philosophers appeal to intuition (as indicated by ‘it seems that’, ‘it appears that’, and ‘it seems to me that’) shows that the practice was accepted and fairly common, even early in the 1800s;
> 4. There is a positive correlation between intuition talk (as indicated by ‘intuit’, ‘intuition, ‘intuitions’, ‘intuitive’, and ‘intuitively’) and appeals to intuition (as indicated by ‘it seems that’, ‘it appears that’, and ‘it seems to me that’).



---

## Method

@article{zollman2015modeling,
  title={Modeling the social consequences of testimonial norms},
  author={Zollman, Kevin JS},
  journal={Philosophical Studies},
  volume={172},
  number={9},
  pages={2371--2383},
  year={2015},
  publisher={Springer}
}


> This paper approaches the problem of testimony from a new direction.
Rather  than  focusing  on  the  epistemic  grounds  for  testimony,  it  considers  the
problem from the perspective of an individual who must choose whom to trust from
a population of many would-be testifiers. A computer simulation is presented which
illustrates that in many plausible situations, those who trust without attempting to
judge the reliability of testifiers outperform those who attempt to seek out the more
reliable members of the community. In so doing, it presents a novel defense for the
credulist  position  that  argues  one  should  trust  testimony  without  considering  the
underlying reliability of the testifier.


@incollection{Dodig-Crnkovic2007-DODP-3,
  year = {2007},
  title = {WHERE DO NEW IDEAS COME FROM? HOW DO THEY EMERGE? - EPISTEMOLOGY AS COMPUTATION},
  editor = {Christian Calude},
  booktitle = {Randomness \& Complexity, from Leibniz to Chaitin},
  author = {Gordana Dodig{-}Crnkovic}
}

> This essay presents arguments for the claim that in the best of all possible worlds (Leibniz) there are sources of unpredictability and creativity for us humans, even given a pancomputational stance. A suggested answer to Chaitin’s questions: “Where do new mathematical and biological ideas come from? How do they emerge?” is that they come from the world and emerge from basic physical (computational) laws. For humans as a tiny subset of the universe, a part of the new ideas comes as the result of the re-configuration and reshaping of already existing elements and another part comes from the outside as a consequence of openness and interactivity of the system. For the universe at large it is randomness that is the source of unpredictability on the fundamental level. In order to be able to completely predict the Universe-computer we would need the Universe-computer itself to compute its next state; as Chaitin already demonstrated there are incompressible truths which means truths that cannot be computed by any other computer but the universe itself. 


@incollection{Wheeler2016-WHEMEA,
  title = {Machine Epistemology and Big Data},
  author = {Gregory Wheeler},
  publisher = {Routledge},
  year = {2016},
  editor = {Lee McIntyre and Alex Rosenburg},
  booktitle = {Routledge Companion to Philosophy of Social Science}
}

> In the age of big data and a machine epistemology that can anticipate, predict, and intervene on events in our lives, the problem once again is that a few individuals possess the knowledge of how to regulate these activities. But the question we face now is not how to share such knowledge more widely, but rather of how to enjoy the public beneﬁts bestowed by this knowledge without freely sharing it. It is not merely personal privacy that is at stake but a range of unsung beneﬁts that come from ignorance and forgetting, traits that are inherently human and integral to the functioning of our society. 

@incollection{HellieForthcoming-HELDLA-2,
  author = {Benj Hellie},
  title = {David Lewis and the Kangaroo: Graphing Philosophical Progress},
  year = {forthcoming},
  booktitle = {Philosophy's Future: The Problem of Philosophical Progress},
  editor = {Russell Blackford and Damien Broderick},
  publisher = {Blackwell}
}

> Data-driven historiography of philosophy looks to objective modeling tools for illumination of the propagation of influence. While the system of David Lewis, the most influential philosopher of our time, raises historiographic puzzles to stymie conventional analytic methods, it proves amenable to data-driven analysis. A striking result is that Lewis only becomes the metaphysician of current legend following the midpoint of his career: his initial project is to frame a descriptive science of mind and meaning; the transition to metaphysics is a rhetorically breathtaking escape from this program's (inevitable) collapse. Understanding this process both aids a more focused debate whether it counts as progress, and also presents novel affordances for partisans on both sides to learn from Lewis's right and wrong steps. 



@article{AshtonForthcoming-ASHITI,
  year = {forthcoming},
  author = {Zoe Ashton and Moti Mizrahi},
  journal = {Erkenntnis},
  title = {Intuition Talk is Not Methodologically Cheap: Empirically Testing the \textquotedblleftReceived Wisdom\textquotedblright About Armchair Philosophy}
}

> The “received wisdom” in contemporary analytic philosophy is that intuition talk is a fairly recent phenomenon, dating back to the 1960s. In this paper, we set out to test two interpretations of this “received wisdom.” The first is that intuition talk is just talk, without any methodological significance. The second is that intuition talk is methodologically significant; it shows that analytic philosophers appeal to intuition. We present empirical and contextual evidence, systematically mined from the JSTOR corpus and HathiTrust’s Digital Library, which provide some empirical support for the second rather than the first hypothesis. Our data also suggest that appealing to intuition is a much older philosophical methodology than the “received wisdom” alleges. We then discuss the implications of our findings for the contemporary debate over philosophical methodology. 


## Dissemination

@article{Bourget2010-BOUPPA,
  year = {2010},
  volume = {24},
  title = {Paperless Philosophy as a Philosophical Method},
  pages = {363--375},
  author = {David Bourget},
  publisher = {Taylor & Francis},
  number = {4},
  journal = {Social Epistemology}
}

> I discuss the prospects for novel communication methods in academic research. I describe communication tools which could enhance the practice of conceptual analysis. 


@unpublished{WiersmaManuscript-WIELPB,
  title = {LogiLogi: Philosophy Beyond the Paper},
  author = {Wybo Wiersma}
}

> This paper sets out to show that philosophy has much to gain from the web, and explores what philosophy on the web might be like. We argue that philosophers usage of the web will undeniably go beyond on-line journals, and the distribution of .pdf files. The failure of historical attempts at making the web work for philosophy are investigated and explained, such as the Xanadu and Discovery projects, and plain web-forums. LogiLogi, a working prototype of a philosophical discussion platform, is then introduced. LogiLogi is different from forums and wikis and tries to overcome their limitations. It does so by aiming for an informal middle-road between good conversations and journal-papers and by providing a form of quick, informal publication, peer-review, and annotation of short philosophical texts. The paper concludes with a tentative analysis of what philosophy on the web should be like, and how LogiLogi is tailored to such a conception of philosophy. 


@article{David2004-DAVTSA-16,
  author = {Nuno David and Maria Marietto and Jaime Sichman and Helder Coelho},
  journal = {Journal of Artificial Societies and Social Simulation},
  volume = {7},
  number = {3},
  title = {The Structure and Logic of Interdisciplinary Research in Agent-Based Social Simulation},
  year = {2004}
}

> This article reports an exploratory survey of the structure of interdisciplinary research in Agent-Based Social Simulation. One hundred and ninety six researchers participated in the survey completing an on-line questionnaire. The questionnaire had three distinct sections, a classification of research domains, a classification of models, and an inquiry into software requirements for designing simulation platforms. The survey results allowed us to disambiguate the variety of scientific goals and modus operandi of researchers with a reasonable level of detail, and to identify a classification of agent-based models used in simulation. In particular, in the interdisciplinary context of social-scientific modelling, agent-based computational modelling and computer engineering, we analyse the extent to which these paradigmatic models seem to be mutually instrumental in the field. We expect that our proposal may improve the viability of submitting, explaining and comparing agent-based simulations in articles, which is an important methodological requirement to consolidate the field. We also expect that it will motivate other proposals that could further validate, extend or change ours, in order to refine the classification with more types of models. 

## Computer Ontology

@article{Brey2005-BRETEA,
  pages = {383--398},
  number = {3-4},
  author = {Philip Brey},
  publisher = {Springer},
  year = {2005},
  volume = {15},
  journal = {Minds and Machines},
  title = {The Epistemology and Ontology of Human-Computer Interaction}
}

> This paper analyzes epistemological and ontological dimensions of Human-Computer Interaction (HCI) through an analysis of the functions of computer systems in relation to their users. It is argued that the primary relation between humans and computer systems has historically been epistemic: computers are used as information-processing and problem-solving tools that extend human cognition, thereby creating hybrid cognitive systems consisting of a human processor and an artificial processor that process information in tandem. In this role, computer systems extend human cognition. Next, it is argued that in recent years, the epistemic relation between humans and computers has been supplemented by an ontic relation. Current computer systems are able to simulate virtual and social environments that extend the interactive possibilities found in the physical environment. This type of relationship is primarily ontic, and extends to objects and places that have a virtual ontology. Increasingly, computers are not just information devices, but portals to worlds that we inhabit. The aforementioned epistemic and ontic relationships are unique to information technology and distinguish human-computer relationships from other human-technology relationships. 

@article{Love2008-LOVAPO-2,
  pages = {27--30},
  number = {2},
  journal = {Philosophy of Management},
  title = {A Philosophy of Maintenance? Engaging with the Concept of Software},
  year = {2008},
  volume = {6},
  publisher = {Libri Publishing},
  author = {David Love}
}

> Although reducing the costs of software maintenance has long been held as an important goal, few researchers have studied software maintenance - except in the context of software design. However, thinking in software design is itself muddled by the frequent confusion over the term ‘software’ and ‘programs’. In this paper we argue for a re-examination of the underlying philosophical foundations of programs, in order to establish software as a phenomenon in its own right. Once we understand the basic structure of software theories, we will be in a better position to understand how theories of software relate to theories of programs. This might finally provide the insight needed to achieve the long awaited reduction in the cost of software maintenance 

@article{Vakarelov2012-VAKTIM,
  title = {The Information Medium},
  pages = {47--65},
  author = {Orlin Vakarelov},
  volume = {25},
  year = {2012},
  journal = {Philosophy and Technology},
  number = {1}
}

> The paper offers the foundations of the theory of information media. Information media are dynamical systems with additional macrostructure of information-carrying states and information-preserving transformations. The paper also defines the notion of information media network as a system of information media connected by information transformations. It is demonstrated that many standard examples of information-containing and processing systems are captured by the general notion of information medium. The paper uses the theory (and informal discussion) of information media to motivate a structural approach to the information in media. The idea is that the notion of information transformation should be regarded as more primitive than the notion of informational state. Thus in information systems, especially in the context of information technology, information is secondary while information transformation is primary 

## Simulation

### Argument

@article{Prakken2012-PRARPV,
  journal = {Artificial Intelligence and Law},
  volume = {20},
  author = {Henry Prakken},
  title = {Reconstructing Popov V. Hayashi in a Framework for Argumentation with Structured Arguments and Dungean Semantics},
  pages = {57--82},
  number = {1},
  year = {2012}
}

> In this article the argumentation structure of the court’s decision in the Popov v. Hayashi case is formalised in Prakken’s (Argument Comput 1:93–124; 2010) abstract framework for argument-based inference with structured arguments. In this framework, arguments are inference trees formed by applying two kinds of inference rules, strict and defeasible rules. Arguments can be attacked in three ways: attacking a premise, attacking a conclusion and attacking an inference. To resolve such conflicts, preferences may be used, which leads to three corresponding kinds of defeat, after which Dung’s (Artif Intell 77:321–357; 1995) abstract acceptability semantics can be used to evaluate the arguments. In the present paper the abstract framework is instantiated with strict inference rules corresponding to first-order logic and with defeasible inference rules for defeasible modus ponens and various argument schemes. The main techniques used in the formal reconstruction of the case are rule-exception structures and arguments about rule validity. Arguments about socio-legal values and the use of precedent cases are reduced to arguments about rule validity. The tree structure of arguments, with explicit subargument relations between arguments, is used to capture the dependency relations between the elements of the court’s decision 



@incollection{David2007-DAVSAF,
  title = {Simulation as Formal and Generative Social Science: The Very Idea},
  booktitle = {Worldviews, Science, and Us: Philosophy and Complexity},
  pages = {266--275},
  author = {Nuno David and Jaime Sichman and Helder Coelho},
  editor = {Carlos Gershenson and Diederik Aerts and Bruce Edmonds},
  year = {2007},
  publisher = {World Scientific}
}

> The formal and empirical-generative perspectives of computation are demonstrated to be inadequate to secure the goals of simulation in the social sciences. Simulation does not resemble formal demonstrations or generative mechanisms that deductively explain how certain models are sufficient to generate emergent macrostructures of interest. The description of scientific practice implies additional epistemic conceptions of scientific knowledge. Three kinds of knowledge that account for a comprehensive description of the discipline were identified: formal, empirical and intentional knowledge. The use of formal conceptions of computation for describing simulation is refuted; the roles of programming languages according to intentional accounts of computation are identified; and the roles of iconographic programming languages and aesthetic machines in simulation are characterized. The roles that simulation and intentional decision making may be able to play in a participative information society are also discussed. 

### Reproducibility 

@article{Angius2011-ANGSTO,
  pages = {323--336},
  number = {2},
  journal = {Minds and Machines},
  year = {2011},
  title = {Scientific Theories of Computational Systems in Model Checking},
  volume = {21},
  author = {Nicola Angius and Guglielmo Tamburrini}
}

> Model checking, a prominent formal method used to predict and explain the behaviour of software and hardware systems, is examined on the basis of reflective work in the philosophy of science concerning the ontology of scientific theories and model-based reasoning. The empirical theories of computational systems that model checking techniques enable one to build are identified, in the light of the semantic conception of scientific theories, with families of models that are interconnected by simulation relations. And the mappings between these scientific theories and computational systems in their scope are analyzed in terms of suitable specializations of the notions of model of experiment and model of data. Furthermore, the extensively mechanized character of model-based reasoning in model checking is highlighted by a comparison with proof procedures adopted by other formal methods in computer science. Finally, potential epistemic benefits flowing from the application of model checking in other areas of scientific inquiry are emphasized in the context of computer simulation studies of biological information processing 



@article{Angius2014-ANGTPO-10,
  year = {2014},
  number = {3},
  pages = {423--439},
  journal = {Philosophy and Technology},
  author = {Nicola Angius},
  volume = {27},
  title = {The Problem of Justification of Empirical Hypotheses in Software Testing}
}

> This paper takes part in the methodological debate concerning the nature and the justification of hypotheses about computational systems in software engineering by providing an epistemological analysis of Software Testing, the practice of observing the programs’ executions to examine whether they fulfil software requirements. Property specifications articulating such requirements are shown to involve falsifiable hypotheses about software systems that are evaluated by means of tests which are likely to falsify those hypotheses. Software Reliability metrics, used to measure the growth of probability that given failures will occur at specified times as new executions are observed, is shown to involve a Bayesian confirmation of falsifiable hypotheses on programs. Coverage criteria, used to select those input values with which the system under test is to be launched, are understood as theory-laden principles guiding software tests, here compared to scientific experiments. Redundant computations, fault seeding models and formal methods used in software engineering to evaluate test results are taken to be instantiations of some epistemological strategies used in scientific experiments to distinguish between valid and non-valid experimental outcomes. The final part of the paper explores the problem, advanced in the context of the philosophy of technology, of defining the epistemological status of software engineering by conceiving it as a scientifically attested technology 




@article{Primiero2015-PRIOMS,
  publisher = {Springer Netherlands},
  year = {2015},
  volume = {192},
  author = {Giuseppe Primiero and Nir Fresco and Luciano Floridi},
  number = {4},
  journal = {Synthese},
  pages = {1199--1220},
  title = {On Malfunctioning Software}
}

> Artefacts do not always do what they are supposed to, due to a variety of reasons, including manufacturing problems, poor maintenance, and normal wear-and-tear. Since software is an artefact, it should be subject to malfunctioning in the same sense in which other artefacts can malfunction. Yet, whether software is on a par with other artefacts when it comes to malfunctioning crucially depends on the abstraction used in the analysis. We distinguish between “negative” and “positive” notions of malfunction. A negative malfunction, or dysfunction, occurs when an artefact token either does not or cannot do what it is supposed to. A positive malfunction, or misfunction, occurs when an artefact token may do what is supposed to but, at least occasionally, it also yields some unintended and undesirable effects. We argue that software, understood as type, may misfunction in some limited sense, but cannot dysfunction. Accordingly, one should distinguish software from other technical artefacts, in view of their design that makes dysfunction impossible for the former, while possible for the latter. 


@article{Fillion2014-FILOTE,
  journal = {Synthese},
  number = {7},
  volume = {191},
  year = {2014},
  author = {Nicolas Fillion and Robert M. Corless},
  title = {On the Epistemological Analysis of Modeling and Computational Error in the Mathematical Sciences},
  pages = {1451--1467}
}

> Interest in the computational aspects of modeling has been steadily growing in philosophy of science. This paper aims to advance the discussion by articulating the way in which modeling and computational errors are related and by explaining the significance of error management strategies for the rational reconstruction of scientific practice. To this end, we first characterize the role and nature of modeling error in relation to a recipe for model construction known as Euler’s recipe. We then describe a general model that allows us to assess the quality of numerical solutions in terms of measures of computational errors that are completely interpretable in terms of modeling error. Finally, we emphasize that this type of error analysis involves forms of perturbation analysis that go beyond the basic model-theoretical and statistical/probabilistic tools typically used to characterize the scientific method; this demands that we revise and complement our reconstructive toolbox in a way that can affect our normative image of science. 


@article{Fetzer1988-FETPVT,
  journal = {Communications of the Acm},
  number = {9},
  pages = {1048--1063},
  year = {1988},
  author = {James H. Fetzer},
  title = {Program Verification: The Very Idea},
  volume = {31}
}

> The notion of program verification appears to trade upon an equivocation. Algorithms, as logical structures, are appropriate subjects for deductive verification. Programs, as causal models of those structures, are not. The success of program verification as a generally applicable and completely reliable method for guaranteeing program performance is not even a theoretical possibility. 


@article{Fetzer1991-FETPAO,
  pages = {197--216},
  number = {2},
  year = {1991},
  author = {James H. Fetzer},
  journal = {Minds and Machines},
  title = {Philosophical Aspects of Program Verification},
  volume = {1}
}

> A debate over the theoretical capabilities of formal methods in computer science has raged for more than two years now. The function of this paper is to summarize the key elements of this debate and to respond to important criticisms others have advanced by placing these issues within a broader context of philosophical considerations about the nature of hardware and of software and about the kinds of knowledge that we have the capacity to acquire concerning their performance. 


@article{David2009-DAVVAV-2,
  year = {2009},
  author = {Nuno David},
  pages = {117--129},
  journal = {Epistemological Aspects of Computer Simulation in the Social Sciences, EPOS 2006, Revised Selected and Invited Papers, Lecture Notes in Artificial Intelligence, Squazzoni, Flaminio (Ed.)},
  volume = {5466},
  title = {Validation and Verification in Social Simulation: Patterns and Clarification of Terminology}
}

> The terms ‘verification’ and ‘validation’ are widely used in science, both in the natural and the social sciences. They are extensively used in simulation, often associated with the need to evaluate models in different stages of the simulation development process. Frequently, terminological ambiguities arise when researchers conflate, along the simulation development process, the technical meanings of both terms with other meanings found in the philosophy of science and the social sciences. This article considers the problem of verification and validation in social science simulation along five perspectives: The reasons to address terminological issues in simulation; the meaning of the terms in the philosophical sense of the problem of “truth”; the observation that some debates about these terms in simulation are inadvertently more terminological than epistemological; the meaning of the terms in the technical context of the simulation development process; and finally, a comprehensive outline of the relation between terminology used in simulation, different types of models used in the development process and different epistemological perspectives. 



@article{Angius2013-ANGAAI,
  pages = {211--226},
  journal = {Minds and Machines},
  title = {Abstraction and Idealization in the Formal Verification of Software Systems},
  number = {2},
  author = {Nicola Angius},
  year = {2013},
  volume = {23}
}

> Questions concerning the epistemological status of computer science are, in this paper, answered from the point of view of the formal verification framework. State space reduction techniques adopted to simplify computational models in model checking are analysed in terms of Aristotelian abstractions and Galilean idealizations characterizing the inquiry of empirical systems. Methodological considerations drawn here are employed to argue in favour of the scientific understanding of computer science as a discipline. Specifically, reduced models gained by Dataion are acknowledged as Aristotelian abstractions that include only data which are sufficient to examine the interested executions. The present study highlights how the need to maximize incompatible properties is at the basis of both Abstraction Refinement, the process of generating a cascade of computational models to achieve a balance between simplicity and informativeness, and the Multiple Model Idealization approach in biology. Finally, fairness constraints, imposed to computational models to allow fair behaviours only, are defined as ceteris paribus conditions under which temporal formulas, formalizing software requirements, acquire the status of law-like statements about the software systems executions 

### Models

@article{Wheeler2013-WHEMMA,
  publisher = {Wiley-Blackwell},
  title = {Models, Models, and Models},
  volume = {44},
  number = {3},
  journal = {Metaphilosophy},
  year = {2013},
  author = {Gregory Wheeler},
  pages = {293--300}
}

> Michael Dummett famously maintained that analytic philosophy was simply philosophy that followed Frege in treating the philosophy of language as the basis for all other philosophy (1978, 441). But one important insight to emerge from computer science is how difficult it is to animate the linguistic artifacts that the analysis of thought produces. Yet, modeling the effects of thought requires a new skill that goes beyond analysis: procedural literacy. Some of the most promising research in philosophy makes use of a variety of modeling techniques that go beyond basic logic and elementary probability theory. What unifies this approach is a focus on what Alan Perlis called procedural literacy. This essay argues that the future spoils in philosophical research will disproportionally go to those who are procedurally literate. 

@article{David2005-DAVTLO-28,
  author = {Nuno David and Jaime Sichman and Helder Coleho},
  journal = {Journal of Artificial Societies and Social Simulation},
  volume = {8},
  number = {4},
  title = {The Logic of the Method of Agent-Based Simulation in the Social Sciences: Empirical and Intentional Adequacy of Computer Programs},
  year = {2005}
}

> The classical theory of computation does not represent an adequate model of reality for simulation in the social sciences. The aim of this paper is to construct a methodological perspective that is able to conciliate the formal and empirical logic of program verification in computer science, with the interpretative and multiparadigmatic logic of the social sciences. We attempt to evaluate whether social simulation implies an additional perspective about the way one can understand the concepts of program and computation. We demonstrate that the logic of social simulation implies at least two distinct types of program verifications that reflect an epistemological distinction in the kind of knowledge one can have about programs. Computer programs seem to possess a causal capability (Fetzer, 1999) and an intentional capability that scientific theories seem not to possess. This distinction is associated with two types of program verification, which we call empirical and intentional verification. We demonstrate, by this means, that computational phenomena are also intentional phenomena, and that such is particularly manifest in agent-based social simulation. Ascertaining the credibility of results in social simulation requires a focus on the identification of a new category of knowledge we can have about computer programs. This knowledge should be considered an outcome of an experimental exercise, albeit not empirical, acquired within a context of limited consensus. The perspective of intentional computation seems to be the only one possible to reflect the multiparadigmatic character of social science in terms of agent-based computational social science. We contribute, additionally, to the clarification of several questions that are found in the methodological perspectives of the discipline, such as the computational nature, the logic of program scalability, and the multiparadigmatic character of agent-based simulation in the social sciences. 


### Examples

@article{Gustafsson2012-PETACS,
  year = {2012},
  number = {3},
  pages = {387--405},
  author = {Johan E. Gustafsson and Martin Peterson},
  title = {A Computer Simulation of the Argument From Disagreement},
  volume = {184},
  journal = {Synthese}
}

> In this paper we shed new light on the Argument from Disagreement by putting it to test in a computer simulation. According to this argument widespread and persistent disagreement on ethical issues indicates that our moral opinions are not influenced by any moral facts, either because no such facts exist or because they are epistemically inaccessible or inefficacious for some other reason. Our simulation shows that if our moral opinions were influenced at least a little bit by moral facts, we would quickly have reached consensus, even if our moral opinions were affected by factors such as false authorities, external political shifts, and random processes. Therefore, since no such consensus has been reached, the simulation gives us increased reason to take seriously the Argument from Disagreement. Our conclusion is however not conclusive; the simulation also indicates what assumptions one has to make in order to reject the Argument from Disagreement. The simulation algorithm we use builds on the work of Hegselmann and Krause (J Artif Soc Social Simul 5(3); 2002, J Artif Soc Social Simul 9(3), 2006). 



@article{Oppenheimer2011-ZALACS,
  title = {A Computationally-Discovered Simplification of the Ontological Argument},
  year = {2011},
  pages = {333--349},
  number = {2},
  volume = {89},
  publisher = {Taylor & Francis},
  journal = {Australasian Journal of Philosophy},
  author = {Paul Oppenheimer and Edward N. Zalta}
}

> The authors investigated the ontological argument computationally. The premises and conclusion of the argument are represented in the syntax understood by the automated reasoning engine PROVER9. Using the logic of definite descriptions, the authors developed a valid representation of the argument that required three non-logical premises. PROVER9, however, discovered a simpler valid argument for God's existence from a single non-logical premise. Reducing the argument to one non-logical premise brings the investigation of the soundness of the argument into better focus. Also, the simpler representation of the argument brings out clearly how the ontological argument constitutes an early example of a ?diagonal argument? and, moreover, one used to establish a positive conclusion rather than a paradox 

### Philosophy of Computing

@article{Dodig-Crnkovic2011-DODSOM,
  number = {2},
  volume = {21},
  pages = {301--322},
  title = {Significance of Models of Computation, From Turing Model to Natural Computation},
  author = {Gordana Dodig{-}Crnkovic},
  journal = {Minds and Machines},
  year = {2011}
}

> The increased interactivity and connectivity of computational devices along with the spreading of computational tools and computational thinking across the fields, has changed our understanding of the nature of computing. In the course of this development computing models have been extended from the initial abstract symbol manipulating mechanisms of stand-alone, discrete sequential machines, to the models of natural computing in the physical world, generally concurrent asynchronous processes capable of modelling living systems, their informational structures and dynamics on both symbolic and sub-symbolic information processing levels. Present account of models of computation highlights several topics of importance for the development of new understanding of computing and its role: natural computation and the relationship between the model and physical implementation, interactivity as fundamental for computational modelling of concurrent information processing systems such as living organisms and their networks, and the new developments in logic needed to support this generalized framework. Computing understood as information processing is closely related to natural sciences; it helps us recognize connections between sciences, and provides a unified approach for modeling and simulating of both living and non-living systems 

@article{Boyer-Kassem2014-BOYLOM,
  author = {Thomas Boyer{-}Kassem},
  number = {4},
  year = {2014},
  journal = {International Studies in the Philosophy of Science},
  pages = {417--436},
  volume = {28},
  title = {Layers of Models in Computer Simulations}
}
   
> I discuss here the definition of computer simulations, and more specifically the views of Humphreys, who considers that an object is simulated when a computer provides a solution to a computational model, which in turn represents the object of interest. I argue that Humphreys's concepts are not able to analyse fully successfully a case of contemporary simulation in physics, which is more complex than the examples considered so far in the philosophical literature. I therefore modify Humphreys's definition of simulation. I allow for several successive layers of computational models, and I discuss the relations that exist between these models, the computer, and the object under study. An aim of my proposal is to clarify the distinction between computational models and numerical methods, and to better understand the representational and the computational functions of models in simulations. 
@article{Muller2008-MLLWAC-2,
  title = {What a Course on Philosophy of Computing is Not},
  number = {1},
  volume = {8},
  pages = {36--38},
  author = {Vincent C. M\"uller},
  journal = {APA Newsletter on Philosophy and Computers},
  year = {2008}
}

> Immanuel Kant famously defined philosophy to be about three questions: “What can I know? What should I do? What can I hope for?” (KrV, B833). I want to suggest that the three questions of our course on the philosophy of computing are: What is computing? What should we do with computing? What could computing do? 



@article{Evens2006-EVEOOO,
  volume = {11},
  title = {Object-Oriented Ontology, or Programming's Creative Fold},
  publisher = {Taylor & Francis},
  number = {1},
  journal = {Angelaki},
  pages = {89--97},
  author = {Aden Evens},
  year = {2006}
}

> This article asks what is creative about the act of programming. Observing that in most programming contexts, each line of code is written with a specific end in mind, it would seem as though there is little room for creativity, as the ends constrain the choices of means. However, there are many features of coding languages that open up creative possibilities. Object-oriented coding environments purport to make programming more about structures that humans might work with and less about features of computers, and other innovative coding techniques, such as RTTI design, suggest that there may be no limit to the creative possibilities of code. 


@incollection{Hernandez-EspinosaForthcoming-HERD-7,
  editor = {},
  publisher = {Springer Verlag},
  title = {Is There Any Real Substance to the Claims for a 'New Computationalism'?},
  year = {forthcoming},
  booktitle = {CiE Computability in Europe 2017},
  author = {Alberto Hernandez{-}Espinosa and Hernandez{-}Quiroz Francisco and Zenil Hector}
}

> 'Computationalism' is a relatively vague term used to describe attempts to apply Turing's model of computation to phenomena outside its original purview: in modelling the human mind, in physics, mathematics, etc. Early versions of computationalism faced strong objections from many (and varied) quarters, from philosophers to practitioners of the aforementioned disciplines. Here we will not address the fundamental question of whether computational models are appropriate for describing some or all of the wide range of processes that they have been applied to, but will focus instead on whether `renovated' versions of the \textit{new computationalism} shed any new light on or resolve previous tensions between proponents and skeptics. We find this, however, not to be the case, because the 'new computationalism' falls short by using limited versions of "traditional computation", or proposing computational models that easily fall within the scope of Turing's original model, or else proffering versions of hypercomputation with its many pitfalls. 


@book{Muller2016-MLLCAP-2,
  title = {Computing and Philosophy: Selected Papers From IACAP 2014},
  author = {Vincent C. M\"uller},
  year = {2016},
  publisher = {Springer}
}

> This volume offers very selected papers from the 2014 conference of the “International Association for Computing and Philosophy” (IACAP) - a conference tradition of 28 years. - - - Table of Contents - 0 Vincent C. Müller: - Editorial - 1) Philosophy of computing - 1 Çem Bozsahin: - What is a computational constraint? - 2 Joe Dewhurst: - Computing Mechanisms and Autopoietic Systems - 3 Vincenzo Fano, Pierluigi Graziani, Roberto Macrelli and Gino Tarozzi: - Are Gandy Machines really local? - 4 Doukas Kapantais: - A refutation of the Church-Turing thesis according to some interpretation of what the thesis says - 5 Paul Schweizer: - In What Sense Does the Brain Compute? - 2) Philosophy of computer science & discovery - 6 Mark Addis, Peter Sozou, Peter C R Lane and Fernand Gobet: - Computational Scientific Discovery and Cognitive Science Theories - 7 Nicola Angius and Petros Stefaneas: - Discovering Empirical Theories of Modular Software Systems. An Algebraic Approach. - 8 Selmer Bringsjord, John Licato, Daniel Arista, Naveen Sundar Govindarajulu and Paul Bello: - Introducing the Doxastically Centered Approach to Formalizing Relevance Bonds in Conditionals - 9 Orly Stettiner: - From Silico to Vitro: - Computational Models of Complex Biological Systems Reveal Real-world Emergent Phenomena - 3) Philosophy of cognition & intelligence - 10 Douglas Campbell: - Why We Shouldn’t Reason Classically, and the Implications for Artificial Intelligence - 11 Stefano Franchi: - Cognition as Higher Order Regulation - 12 Marcello Guarini: - Eliminativisms, Languages of Thought, & the Philosophy of Computational Cognitive Modeling - 13 Marcin Miłkowski: - A Mechanistic Account of Computational Explanation in Cognitive Science and Computational Neuroscience - 14 Alex Tillas: - Internal supervision & clustering: - A new lesson from ‘old’ findings? - 4) Computing & society - 15 Vasileios Galanos: - Floridi/Flusser: - Parallel Lives in Hyper/Posthistory - 16 Paul Bello: - Machine Ethics and Modal Psychology - 17 Marty J. Wolf and Nir Fresco: - My Liver Is Broken, Can You Print Me a New One? - 18 Marty J. Wolf, Frances Grodzinsky and Keith W. Miller: - Robots, Ethics and Software – FOSS vs. Proprietary Licenses 

@article{Dodig-Crnkovic2008-DODKGA,
  number = {2},
  year = {2008},
  volume = {6},
  author = {Gordana Dodig{-}Crnkovic},
  journal = {Journal of Systemics, Cybernetics and Informatics},
  title = {Knowledge Generation as Natural Computation}
}

> Knowledge generation can be naturalized by adopting computational model of cognition and evolutionary approach. In this framework knowledge is seen as a result of the structuring of input data (data → information → knowledge) by an interactive computational process going on in the agent during the adaptive interplay with the environment, which clearly presents developmental advantage by increasing agent’s ability to cope with the situation dynamics. This paper addresses the mechanism of knowledge generation, a process that may be modeled as natural computation in order to be better understood and improved. 




@incollection{Dodig-Crnkovic2007-DODP-3,
  author = {Gordana Dodig{-}Crnkovic},
  booktitle = {Randomness \& Complexity, from Leibniz to Chaitin},
  title = {WHERE DO NEW IDEAS COME FROM? HOW DO THEY EMERGE? - EPISTEMOLOGY AS COMPUTATION},
  year = {2007},
  editor = {Christian Calude}
}

> This essay presents arguments for the claim that in the best of all possible worlds (Leibniz) there are sources of unpredictability and creativity for us humans, even given a pancomputational stance. A suggested answer to Chaitin’s questions: “Where do new mathematical and biological ideas come from? How do they emerge?” is that they come from the world and emerge from basic physical (computational) laws. For humans as a tiny subset of the universe, a part of the new ideas comes as the result of the re-configuration and reshaping of already existing elements and another part comes from the outside as a consequence of openness and interactivity of the system. For the universe at large it is randomness that is the source of unpredictability on the fundamental level. In order to be able to completely predict the Universe-computer we would need the Universe-computer itself to compute its next state; as Chaitin already demonstrated there are incompressible truths which means truths that cannot be computed by any other computer but the universe itself. 

@article{Eden2007-EDETPO,
  number = {2},
  author = {Amnon H. Eden},
  title = {Three Paradigms of Computer Science},
  volume = {17},
  pages = {135--167},
  year = {2007},
  journal = {Minds and Machines},
  publisher = {Springer}
}

> We examine the philosophical disputes among computer scientists concerning methodological, ontological, and epistemological questions: Is computer science a branch of mathematics, an engineering discipline, or a natural science? Should knowledge about the behaviour of programs proceed deductively or empirically? Are computer programs on a par with mathematical objects, with mere data, or with mental processes? We conclude that distinct positions taken in regard to these questions emanate from distinct sets of received beliefs or paradigms within the discipline: – The rationalist paradigm, which was common among theoretical computer scientists, defines computer science as a branch of mathematics, treats programs on a par with mathematical objects, and seeks certain, a priori knowledge about their ‘correctness’ by means of deductive reasoning. – The technocratic paradigm, promulgated mainly by software engineers and has come to dominate much of the discipline, defines computer science as an engineering discipline, treats programs as mere data, and seeks probable, a posteriori knowledge about their reliability empirically using testing suites. – The scientific paradigm, prevalent in the branches of artificial intelligence, defines computer science as a natural (empirical) science, takes programs to be entities on a par with mental processes, and seeks a priori and a posteriori knowledge about them by combining formal deduction and scientific experimentation. We demonstrate evidence corroborating the tenets of the scientific paradigm, in particular the claim that program-processes are on a par with mental processes. We conclude with a discussion in the influence that the technocratic paradigm has been having over computer science. 

@article{Iwanska1993-IWALRI,
  journal = {Minds and Machines},
  volume = {3},
  author = {Lucja Iwaska},
  pages = {475--510},
  number = {4},
  title = {Logical Reasoning in Natural Language: It is All About Knowledge},
  year = {1993}
}

> A formal, computational, semantically clean representation of natural language is presented. This representation captures the fact that logical inferences in natural language crucially depend on the semantic relation of entailment between sentential constituents such as determiner, noun, adjective, adverb, preposition, and verb phrases.The representation parallels natural language in that it accounts for human intuition about entailment of sentences, it preserves its structure, it reflects the semantics of different syntactic categories, it simulates conjunction, disjunction, and negation in natural language by computable operations with provable mathematical properties, and it allows one to represent coordination on different syntactic levels. 

@article{Rapaport1999-RAPIIS,
  year = {1999},
  volume = {82},
  title = {Implementation Is Semantic Interpretation},
  pages = {109--130},
  number = {1},
  publisher = {The Hegeler Institute},
  author = {William J. Rapaport},
  journal = {The Monist}
}

> What is the computational notion of "implementation"? It is not individuation, instantiation, reduction, or supervenience. It is, I suggest, semantic interpretation. The online version differs from the published version in being a bit longer and going into a bit more detail. 






---

@unpublished{WadlerManuscript-WADPAP,
  author = {Philip Wadler},
  title = {Proofs Are Programs: 19th Century Logic and 21st Century Computing}
}

> As the 19th century drew to a close, logicians formalized an ideal notion of proof. They were driven by nothing other than an abiding interest in truth, and their proofs were as ethereal as the mind of God. Yet within decades these mathematical abstractions were realized by the hand of man, in the digital stored-program computer. How it came to be recognized that proofs and programs are the same thing is a story that spans a century, a chase with as many twists and turns as a thriller. At the end of the story is a new principle for designing programming languages that will guide computers into the 21st century. For my money, Gentzen’s natural deduction and Church’s lambda calculus are on a par with Einstein’s relativity and Dirac’s quantum physics for elegance and insight. And the maths are a lot simpler. I want to show you the essence of these ideas. I’ll need a few symbols, but not too many, and I’ll explain as I go along. To simplify, I’ll present the story as we understand it now, with some asides to fill in the history. First, I’ll introduce Gentzen’s natural deduction, a formalism for proofs. Next, I’ll introduce Church’s lambda calculus, a formalism for programs. Then I’ll explain why proofs and programs are really the same thing, and how simplifying a proof corresponds to executing a program. Finally, I’ll conclude with a look at how these principles are being applied to design a new generation of programming languages, particularly mobile code for the Internet. 